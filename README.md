# Taxis_DataAnalysis_MLClassificationModels

## This notebook analyzes a taxi dataset by performing data cleaning, preprocessing, visualization, and machine learning modeling.

*************************************
1Ô∏è‚É£ **Data Cleaning & Preprocessing**

‚úÖ Handles missing values using SimpleImputer.

‚úÖ Detects and removes duplicate values.

‚úÖ Encodes categorical features using LabelEncoder.

‚úÖ Applies TF-IDF & SVD on text-based features (pickup_zone, dropoff_zone).

‚úÖ Uses Power and Quantile transformations to normalize numerical features.
************************************************
2Ô∏è‚É£ **Data Visualization**

‚úÖ Heatmaps to check missing values.

‚úÖ Box plots to understand numerical feature distributions.

‚úÖ Donut & Bar charts for categorical data insights.
****************************************************
3Ô∏è‚É£ **Machine Learning Models**

‚úÖ Compares LightGBM, CatBoost, and RandomForest classifiers.

‚úÖ Uses Cross-Validation with ROC AUC as an evaluation metric.

‚úÖ Prints model performance results to identify the best classifier for predicting payment type in the taxi dataset.
*********************************************
4Ô∏è‚É£ **Key Libraries Used**

- pandas, numpy ‚Üí Data processing.
  
- matplotlib, seaborn, plotly ‚Üí Data visualization.
  
- SimpleImputer, LabelEncoder ‚Üí Data preprocessing.
  
- TF-IDF, SVD ‚Üí Text feature transformation.
  
- PowerTransformer, QuantileTransformer ‚Üí Feature scaling.
- 
- LightGBM, CatBoost, RandomForest ‚Üí Machine learning models.
********************************************************
üîπ **Summary**
‚úî Cleans and preprocesses the taxi dataset.
‚úî Visualizes key insights with different plots.
‚úî Compares ML models to predict payment type.

**********************************************************
NOTE:
**CatBoost performs best likely because it handles categorical features and complex relationships efficiently, reducing overfitting.**

**LightGBM performs competitively but might need more careful hyperparameter tuning to reach the highest performance.**

**RandomForest shows the lowest ROC AUC among the three, possibly due to its more basic averaging approach and inability to capture intricate feature interactions as effectively as the boosting algorithms.**

**Each model's performance can also be influenced by the specifics of your dataset (e.g., noise level, feature distribution, presence of outliers), so fine-tuning and feature engineering play significant roles.**
